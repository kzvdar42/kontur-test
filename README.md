# Kontur - Тестовое задание

**Автор**: Владислав Кулейкин

## Основная информация
Это решение тестового задания от компании Контур на стажировку по направлениям CV/NLP в 2020 году.

## Как запустить
Всё довольно просто, запускаете `TestClassificator.ipynb`, там в первой же ячейке можно задать:
* `vocab_size` - размер словаря токенизатора
* `train_data_path` - путь до train файла
* `test_data_path` - путь до test файла
* `tokenizer_path` - путь где хранить токенизатор
* `DatasetClass` - какой класс использовать для датасета (пояснится в пункте про архитектуру)
* `ModelClass` - какой класс модели использовать

После изменения этих параметров можно смело жать *Run All Cells*.

## Данные
Так же хотелось бы сделать пару комментариев по полученным данным для тренировки:
* **Не нормализированные классы** - более 95% данных с негативной классификацией
* **Нет дубликатов** - не было найдено дублирующих значений
* Для большинства названий нет позитивного примера
* В тестовом датасете нет символов, что бы не встречались в тренировочном датасете (важно, так как тоекнизатор работает на уровне символов).

## Обучение модели

### Обработка данных
Так как данные несбалансированные, при обучении модели я делаю семплы из данных для тренировки, так, чтобы количество семплов с позитивной классификацией совпадало с количеством семплов с негативной классификацией. И повторяю эту операцию перед каждой эпохой тренировки.

Для токенизации я использовал библиотеку `youtokentome` что создаёт Byte Pair Encoding (BPE), обучается на указанном тексте и предлагает выбрать размер словаря.

Перед обучением train данные были разделены на сет для обучения и валидации в отношении 80/20.


### Архитектура
Для решения задачи я испробовал несколько моделей (LSTM, CNN, TransformerEncoder), но остановился на LSTM, так как он показывал самый большой рост и показатель по accuracy на тестовых данных.

### Гиперпараметры
Список параметров использованных при обучении модели:
* vocab_size - 184 (как и количество символов в тексте, но тут самые редкие символы опущены в пользу более популярных частиц)
* emb_size - 64
* hidden_size - 128
* num_layers - 2
* dropout - 0.1
* learning rate - 0.001 (с небольшим уменьшением каждые * две эпохи)
* n_epochs - 12
* optimizer - Adam
* criterion - CrossEntropyLoss

### Результаты
Здесь вы можете увидеть график изменения accuracy и loss во время обучения:
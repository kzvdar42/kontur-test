{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.notion.so/c2cc4e28d7ad46de912c9996f52276da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 184\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import youtokentome as yttm\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "from models import LstmClassificator, LstmPackedClassificator, DoubleLstmClassificator\n",
    "from train_model import train_model, run_model, save_model\n",
    "from data_utils import create_tokenizer, TextDataset, TextWithLengthDataset, SplitTextDataset, Dataloaders\n",
    "\n",
    "vocab_size = 184 # 2 ** 7 # 184\n",
    "PAD_TOKEN = 0\n",
    "UNK_TOKEN = 1\n",
    "BOS_TOKEN = 2\n",
    "EOS_TOKEN = 3\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "BATCH_SIZE = 32 # 64\n",
    "\n",
    "# Choose dataset\n",
    "data_path = 'data'\n",
    "train_data_path = os.path.join(data_path, 'train_data.tsv')\n",
    "test_data_path = os.path.join(data_path, 'test_data.tsv')\n",
    "# Tokenizer model path.\n",
    "tokenizer_path = os.path.join('tokenizer', f'v{vocab_size}.tokenizer')\n",
    "\n",
    "DatasetClass = TextWithLengthDataset\n",
    "ModelClass = LstmPackedClassificator\n",
    "\n",
    "print(f'Vocab size: {vocab_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, 0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available(), torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alex\\Anaconda3\\envs\\ml\\lib\\site-packages\\numpy\\lib\\arraysetops.py:569: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_csv(train_data_path, sep='\\t', index_col=0)\n",
    "test_data = pd.read_csv(test_data_path, sep='\\t', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(182876, 3800327)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_inds = train_data.index[train_data.answer == True]\n",
    "false_inds = train_data.index[train_data.answer == False]\n",
    "len(true_inds), len(false_inds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained tokenizer...\n"
     ]
    }
   ],
   "source": [
    "data_for_tokenizer = [train_data['ru_name'], train_data['eng_name'],\n",
    "                      test_data['ru_name'], test_data['eng_name']]\n",
    "tokens_ids = {'pad_id':PAD_TOKEN, 'unk_id':UNK_TOKEN,\n",
    "              'bos_id':BOS_TOKEN, 'eos_id':EOS_TOKEN}\n",
    "tokenizer = create_tokenizer(tokenizer_path, data_for_tokenizer, vocab_size, tokens_ids)\n",
    "del data_for_tokenizer, tokens_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 260000 \n",
      "Val: 72000\n"
     ]
    }
   ],
   "source": [
    "ratio = 0.8\n",
    "# TODO: Distribute true/false evenly?\n",
    "msk = np.random.rand(len(train_data)) < ratio\n",
    "\n",
    "train_sample_size = 130000 # len(true_inds) // 2\n",
    "test_sample_size  = 36000 # len(true_inds) // 4\n",
    "\n",
    "\n",
    "datasets = {\n",
    "    'train': DatasetClass(train_data[msk],  tokenizer, sample_size=train_sample_size),\n",
    "    'val':   DatasetClass(train_data[~msk], tokenizer, sample_size=test_sample_size),\n",
    "}\n",
    "\n",
    "print('Train:', len(datasets['train']),\n",
    "      '\\nVal:', len(datasets['val']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36316, 760075)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_name = 'val'\n",
    "dataset_true_inds  = datasets[dataset_name].data.index[datasets[dataset_name].data.answer == True]\n",
    "dataset_false_inds = datasets[dataset_name].data.index[datasets[dataset_name].data.answer == False]\n",
    "len(dataset_true_inds), len(dataset_false_inds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<BOS>',\n",
       " '▁общество',\n",
       " '▁с',\n",
       " '▁ограниченной',\n",
       " '▁ответственностью',\n",
       " '▁\"',\n",
       " 'в',\n",
       " 'о',\n",
       " 'ст',\n",
       " 'о',\n",
       " 'ч',\n",
       " 'н',\n",
       " 'а',\n",
       " 'я',\n",
       " '▁',\n",
       " 'р',\n",
       " 'е',\n",
       " 'м',\n",
       " 'о',\n",
       " 'н',\n",
       " 'т',\n",
       " 'но',\n",
       " '-',\n",
       " 'ст',\n",
       " 'ро',\n",
       " 'и',\n",
       " 'т',\n",
       " 'е',\n",
       " 'л',\n",
       " 'ь',\n",
       " 'н',\n",
       " 'а',\n",
       " 'я',\n",
       " '▁',\n",
       " 'ко',\n",
       " 'м',\n",
       " 'п',\n",
       " 'ани',\n",
       " 'я',\n",
       " '\"',\n",
       " '<EOS>',\n",
       " '▁',\n",
       " 'e',\n",
       " 'a',\n",
       " 'st',\n",
       " '▁',\n",
       " 'r',\n",
       " 'e',\n",
       " 'p',\n",
       " 'a',\n",
       " 'i',\n",
       " 'r',\n",
       " '▁',\n",
       " 'b',\n",
       " 'u',\n",
       " 'i',\n",
       " 'l',\n",
       " 'd',\n",
       " 'in',\n",
       " 'n',\n",
       " 'g',\n",
       " '▁company',\n",
       " '▁limited',\n",
       " '<EOS>']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = np.random.randint(len(datasets['train']))\n",
    "datasets['train'].decode(datasets['train'][idx][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iterators = Dataloaders(datasets, pad_token=PAD_TOKEN,\n",
    "                             batch_size=BATCH_SIZE, shuffle=False,\n",
    "                             num_workers=0, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import ModuleWithWeightsInit, PoolingModule, Attn\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModelClass(vocab_size=tokenizer.vocab_size(),\n",
    "                                emb_size=64,\n",
    "                                hidden_size=128,\n",
    "                                num_classes=2,\n",
    "                                num_layers=2,\n",
    "                                padding_idx=PAD_TOKEN,\n",
    "                                dropout=0.1,\n",
    "                                fc_dims = 1024,\n",
    "                                pool_attn=False,\n",
    "                                pool_max=True,\n",
    "                                pool_min=True,\n",
    "                         )\n",
    "model = model.to(device)\n",
    "stats = None\n",
    "model_save_path=os.path.join('models', f'packed_{vocab_size}_64_128.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model, stats = torch.load(model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 1\n",
    "learning_rate = 0.001\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = StepLR(optimizer, step_size=2, gamma=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ Epoch 0; lr: 0.00100 ------------\n",
      "Resample data from datasets...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "723b454f6a514b48877e67af70346e1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Train Epoch #0 ', max=8125.0, style=ProgressStyle(descrip…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Acc: 0.8818\n",
      "Train Loss: 0.2881\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09850c5e3d4c4ac09fff45100ca8693b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Val Epoch #0 ', max=2250.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Val Acc: 0.9307\n",
      "Val Loss: 0.1936\n"
     ]
    }
   ],
   "source": [
    "stats = train_model(model, n_epochs, data_iterators, criterion, optimizer,\n",
    "                    scheduler=scheduler, stats=stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': {'acc': [0.8817538461538461], 'loss': [0.28805430213717315]},\n",
       " 'val': {'acc': [0.9306805555555555], 'loss': [0.19356126191218695]}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.title('Accuracy')\n",
    "plt.plot(stats['val']['acc'],   '-b',  label='Val')\n",
    "plt.plot(stats['train']['acc'], '--g', label='Train')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.title('Loss')\n",
    "plt.plot(stats['val']['loss'],   '-b',  label='Val')\n",
    "plt.plot(stats['train']['loss'], '--g', label='Train')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on full validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = DatasetClass(train_data[~msk], tokenizer, normalize=False)\n",
    "test_iterator = Dataloaders({'test':test_dataset}, pad_token=PAD_TOKEN,\n",
    "                                             batch_size=128, shuffle=False, num_workers=0)\n",
    "is_train_phase = False\n",
    "desc = \"Test model \"\n",
    "test_results = run_model(model, test_iterator['test'], is_train_phase, criterion, optimizer, desc=desc)\n",
    "\n",
    "# max 98.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, data_iterator, desc=\"\"):\n",
    "    \"\"\"Run the model through the given data.\n",
    "\n",
    "    :param model: model to run\n",
    "    :param data_iterator: iterator for data\n",
    "    :param is_train_phase: if `True` run model in train mode\n",
    "    :param desc: description for the status printing\n",
    "    :returns: list of predictions\n",
    "    \"\"\"\n",
    "    # Get device from the model\n",
    "    device = next(model.parameters()).get_device()\n",
    "    # Put the model in eval mode.\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    pbar = tqdm(total=len(data_iterator), desc=desc, position=0, leave=True)\n",
    "    for i, data in enumerate(data_iterator):\n",
    "        for j, tensor in enumerate(data):\n",
    "            data[j] = tensor.to(device, non_blocking=True)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        with torch.set_grad_enabled(False):\n",
    "            out = model(*data)\n",
    "\n",
    "        predictions.extend(out)\n",
    "        pbar.update(1)\n",
    "    pbar.close()\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from data_utils import pad_sequence\n",
    "\n",
    "class TestTextDataset(Dataset):\n",
    "    \"\"\"Custom dataset for train data.\"\"\"\n",
    "\n",
    "    __output_types = {'id': yttm.OutputType.ID,\n",
    "                      'subword': yttm.OutputType.SUBWORD}\n",
    "\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        # TODO: Change `eos` token to `sep` token\n",
    "        self._sep_token = self.tokenize(\"\", eos=True)[0]\n",
    "\n",
    "    def tokenize(self, sentence, output_type='id', **kwargs):\n",
    "        \"\"\"Tokenize the sentence.\n",
    "        :param s: the sentence to tokenize\n",
    "        :param output_type: either 'id' or 'subword' for corresponding output\n",
    "        :return: tokenized sentence\"\"\"\n",
    "        if not isinstance(sentence, str):\n",
    "            return [self.tokenize(sent, output_type, **kwargs) for sent in sentence]\n",
    "        return self.tokenizer.encode(sentence.lower().strip(),\n",
    "                                     output_type=self.__output_types[output_type], **kwargs)\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        return [self.tokenizer.id_to_subword(token) for token in tokens]\n",
    "    \n",
    "    @staticmethod\n",
    "    def _my_collate(batch, pad_token):\n",
    "        src, src_lens = zip(*batch)\n",
    "        src = [Tensor(s) for s in src]\n",
    "        src = pad_sequence(src, batch_first=True, padding_value=pad_token).long()\n",
    "        src_lens = Tensor(src_lens).long()\n",
    "        return [src, src_lens]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        ru  = self.tokenize(self.data['ru_name'].iloc[idx],  bos=True)\n",
    "        eng = self.tokenize(self.data['eng_name'].iloc[idx], eos=True)\n",
    "        src = ru + [self._sep_token] + eng\n",
    "        src_len = len(src)\n",
    "        return src, src_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = {'test': TestTextDataset(test_data,  tokenizer)}\n",
    "test_iterator = Dataloaders(test_dataset, pad_token=PAD_TOKEN,\n",
    "                                          batch_size=128, shuffle=False, num_workers=0)\n",
    "\n",
    "res = predict(model, test_iterator['test'], desc='Predict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_test = []\n",
    "for xi in tqdm(res):\n",
    "    res_test.append(xi.topk(1).indices.squeeze().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame(res_test, columns=['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv(' result.tsv', sep='\\t')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
